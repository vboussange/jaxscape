{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a93e95",
   "metadata": {},
   "source": [
    "# Inverse Landscape Modeling with JAXScape\n",
    "\n",
    "## Overview\n",
    "\n",
    "Landscape genetics aims to understand how landscape structure influences gene flow and genetic differentiation among populations. Traditional approaches assume resistance values for different habitat types, but **inverse modeling** offers a data-driven alternative: we infer landscape resistance patterns directly from observed genetic data.\n",
    "\n",
    "This notebook demonstrates how to fit a neural network-based resistance model to genetic differentiation (Fst) measurements using JAXScape's differentiable distance metrics and gradient-based optimization. Rather than making *a priori* assumptions about which land-cover types facilitate or impede movement, we let the data reveal these patterns through automatic differentiation and iterative refinement. The approach leverages JAX's computational efficiency to make inverse modeling tractable for realistic landscape-scale problems, opening new possibilities for evidence-based conservation planning and understanding species-landscape relationships.\n",
    "\n",
    "### Prerequisites:\n",
    "```bash\n",
    "pip install jaxscape equinox optimistix rioxarray geopandas scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import equinox as eqx\n",
    "from equinox import nn\n",
    "import optimistix as optx\n",
    "\n",
    "from jaxscape import GridGraph, ResistanceDistance, LCPDistance\n",
    "from jaxscape.solvers import CholmodSolver\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9326b97",
   "metadata": {},
   "source": [
    "### Configuration Parameters\n",
    "\n",
    "The analysis requires several key hyperparameters. The **coarsening factor** controls spatial downsampling to reduce computational cost while preserving landscape patterns. We use a **Cholesky solver** for efficient computation of resistance distances on large graphs. The **distance metric** defaults to ResistanceDistance, though LCPDistance can be substituted for experimentation. Finally, **max steps** limits the number of optimization iterations to ensure convergence within reasonable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008aa775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths (adjust to your data location)\n",
    "LANDCOVER_PATH = '../data/cesar/landcover_7855.tif'\n",
    "SITE_METADATA_PATH = '../data/cesar/cesar_site_metadata.gpkg'\n",
    "GENETIC_DISTANCES_PATH = '../data/cesar/cesar_genetic_distances.npy'\n",
    "\n",
    "# Model configuration\n",
    "COARSENING_FACTOR = 10  # Spatial downsampling for computational efficiency\n",
    "SOLVER = CholmodSolver()  # Fast linear solver for large graphs\n",
    "DISTANCE_FUN = ResistanceDistance(solver=SOLVER)  # Effective resistance distance\n",
    "MAX_STEPS = 500  # Maximum optimization iterations\n",
    "\n",
    "# Alternative distance metric for experimentation:\n",
    "# DISTANCE_FUN = LCPDistance()  # Least-cost path distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273f760",
   "metadata": {},
   "source": [
    "## Load and Visualize Input Data\n",
    "\n",
    "The analysis requires three key datasets. The **land-cover raster** from ESA WorldCover provides habitat type classifications across the study area. **Site metadata** contains the geographic coordinates of sampling locations where genetic data were collected. The **genetic distance matrix** stores pairwise Fst values (or similar differentiation metrics) quantifying how genetically distinct populations are at each pair of sites. This genetic differentiation reflects the cumulative effect of landscape resistance on gene flow, which our model will learn to predict from the spatial pattern of land-cover types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load land-cover raster\n",
    "predictor_raster = rioxarray.open_rasterio(\n",
    "    LANDCOVER_PATH, \n",
    "    mask_and_scale=True\n",
    ")\n",
    "\n",
    "# Load site metadata (geographic locations)\n",
    "site_metadata = gpd.read_file(SITE_METADATA_PATH)\n",
    "site_gdf = site_metadata.to_crs(epsg=7855)  # Reproject to Australian GDA2020\n",
    "\n",
    "# Load genetic distance matrix\n",
    "genetic_distances = np.load(GENETIC_DISTANCES_PATH)\n",
    "\n",
    "print(f\"Land-cover raster shape: {predictor_raster['band_1'].shape}\")\n",
    "print(f\"Number of sites: {len(site_gdf)}\")\n",
    "print(f\"Genetic distance matrix shape: {genetic_distances.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da822373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize land-cover with sampling sites\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "predictor_raster[\"band_1\"].plot(ax=ax, cmap=\"tab20\", add_colorbar=True)\n",
    "ax.scatter(\n",
    "    site_gdf.geometry.x, \n",
    "    site_gdf.geometry.y, \n",
    "    c=\"red\", \n",
    "    s=100, \n",
    "    edgecolor='white',\n",
    "    linewidth=2,\n",
    "    label=\"Sampling Sites\",\n",
    "    zorder=10\n",
    ")\n",
    "ax.set_title(\"Land-Cover Map with Sampling Sites\", fontsize=14, pad=20)\n",
    "ax.legend(loc='upper right', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sites: {', '.join(site_gdf['site_name'].values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de563334",
   "metadata": {},
   "source": [
    "## Prepare Features and Targets\n",
    "\n",
    "The land-cover data requires several preprocessing steps to create suitable model inputs. First, we **compress class IDs** by converting sparse WorldCover classes to a contiguous range 0..K-1, making one-hot encoding more efficient. Next, we **one-hot encode** these categorical classes to create binary feature vectors that the neural network can process. We then apply **coarsening via mean pooling** to downsample the raster while preserving the proportional composition of land-cover classes within each coarsened cell—this captures habitat heterogeneity rather than forcing a single discrete class per pixel. Finally, we **map site coordinates to grid nodes** by associating each sampling location with its nearest coarsened grid cell. This preprocessing pipeline balances computational efficiency with preserving the spatial detail most relevant to understanding gene flow patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature_targets(predictor_raster, site_gdf, coarsening_factor):\n",
    "    \"\"\"Process land-cover and create model inputs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    features_onehot_coarse : array\n",
    "        One-hot encoded land-cover features after coarsening (H, W, K)\n",
    "    unique_classes : array\n",
    "        Original WorldCover class values\n",
    "    target_nodes : array\n",
    "        Node indices for sampling sites\n",
    "    grid : GridGraph\n",
    "        Reference grid for node indexing\n",
    "    feature_da : xarray.DataArray\n",
    "        Coarsened feature raster with coordinates\n",
    "    \"\"\"\n",
    "    # Compress WorldCover classes to contiguous IDs\n",
    "    raw_band = np.asarray(predictor_raster[\"band_1\"])\n",
    "    unique_vals, inverse = np.unique(raw_band.ravel(), return_inverse=True)\n",
    "    class_ids = inverse.reshape(raw_band.shape).astype(np.int32)\n",
    "    features_categorical = jnp.array(class_ids).squeeze()\n",
    "    unique_classes = jnp.array(unique_vals)\n",
    "    \n",
    "    print(f\"Found {len(unique_vals)} unique land-cover classes\")\n",
    "    \n",
    "    # One-hot encode: (H, W) -> (H, W, K)\n",
    "    features_onehot = jax.nn.one_hot(\n",
    "        features_categorical, \n",
    "        num_classes=len(unique_vals)\n",
    "    )\n",
    "    \n",
    "    # Reorder for coarsening: (H, W, K) -> (K, H, W)\n",
    "    features_onehot = jnp.moveaxis(features_onehot, -1, 0)\n",
    "    \n",
    "    # Coarsen using mean pooling (preserves class composition)\n",
    "    coords = {\n",
    "        \"band\": np.arange(features_onehot.shape[0]),\n",
    "        \"y\": predictor_raster.y.values,\n",
    "        \"x\": predictor_raster.x.values,\n",
    "    }\n",
    "    feature_da = xr.DataArray(\n",
    "        features_onehot,\n",
    "        coords=coords,\n",
    "        dims=(\"band\", \"y\", \"x\"),\n",
    "    )\n",
    "    feature_da = feature_da.coarsen(\n",
    "        x=coarsening_factor, \n",
    "        y=coarsening_factor, \n",
    "        boundary=\"trim\"\n",
    "    ).mean()\n",
    "    \n",
    "    # Back to (H, W, K)\n",
    "    features_onehot_coarse = jnp.moveaxis(feature_da.data, 0, -1)\n",
    "    print(f\"Coarsened feature shape: {features_onehot_coarse.shape}\")\n",
    "    \n",
    "    # Map site coordinates to coarsened grid indices\n",
    "    x_idx = jnp.array([\n",
    "        int(np.argmin(np.abs(feature_da.x.values - x))) \n",
    "        for x in site_gdf.geometry.x.values\n",
    "    ])\n",
    "    y_idx = jnp.array([\n",
    "        int(np.argmin(np.abs(feature_da.y.values - y))) \n",
    "        for y in site_gdf.geometry.y.values\n",
    "    ])\n",
    "    \n",
    "    # Create reference grid for node indexing\n",
    "    grid = GridGraph(\n",
    "        jnp.ones((feature_da.x.size, feature_da.y.size)), \n",
    "        fun=lambda x, y: (x + y) / 2\n",
    "    )\n",
    "    target_nodes = grid.coord_to_index(x_idx, y_idx)\n",
    "    \n",
    "    return features_onehot_coarse, unique_classes, target_nodes, grid, feature_da\n",
    "\n",
    "# Process features\n",
    "features_onehot, unique_classes, target_nodes, ref_grid, coarse_feature_da = prepare_feature_targets(\n",
    "    predictor_raster, site_gdf, COARSENING_FACTOR\n",
    ")\n",
    "\n",
    "print(f\"Target nodes (site indices): {target_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coarsened land-cover with sites\n",
    "node_coords = ref_grid.index_to_coord(target_nodes)\n",
    "x_indices, y_indices = node_coords[:, 0], node_coords[:, 1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.imshow(features_onehot.argmax(axis=-1), cmap=\"tab20\")\n",
    "ax.scatter(\n",
    "    x_indices, y_indices, \n",
    "    c=\"blue\", \n",
    "    s=150, \n",
    "    edgecolor='white',\n",
    "    linewidth=2,\n",
    "    label=\"Sites\",\n",
    "    zorder=10\n",
    ")\n",
    "\n",
    "# Annotate sites\n",
    "for xi, yi, name in zip(x_indices, y_indices, site_gdf[\"site_name\"].values):\n",
    "    ax.text(\n",
    "        int(xi), int(yi) - 5,\n",
    "        str(name),\n",
    "        color=\"black\",\n",
    "        fontsize=10,\n",
    "        fontweight='bold',\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.9, edgecolor=\"black\", pad=2),\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Coarsened Land-Cover with Site Locations\", fontsize=14, pad=20)\n",
    "ax.legend(loc='upper right', fontsize=12)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31611502",
   "metadata": {},
   "source": [
    "## Define the Resistance Model\n",
    "\n",
    "We build a **neural network** that maps one-hot land-cover features to positive resistance values according to the transformation $\\text{resistance} = \\exp(\\text{NN}(\\text{features})) + \\epsilon$. The architecture consists of a K-dimensional one-hot vector as **input** (representing land-cover classes), followed by **two hidden layers** with ReLU activation and 16 hidden units each, culminating in a **single output** value that is exponentiated to ensure positive resistance. This model is applied pixel-wise via `vmap` to produce a complete resistance surface across the landscape.\n",
    "\n",
    "The design reflects several important considerations. The **small network** (16 hidden units) helps prevent overfitting when training on limited genetic data, forcing the model to learn parsimonious resistance patterns. The **exponential activation** on the output ensures all predicted resistance values are physically meaningful (positive), which is required for well-defined distance metrics. The **one-hot input encoding** allows the model to learn distinct resistance values for each land-cover type while also capturing their interactions through the hidden layers, providing more flexibility than simple categorical lookup tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9644ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes: int, seed: int = 1) -> tuple:\n",
    "    \"\"\"Build neural resistance model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_classes : int\n",
    "        Number of land-cover classes\n",
    "    seed : int\n",
    "        Random seed for initialization\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model : eqx.Module\n",
    "        Complete model\n",
    "    params : pytree\n",
    "        Trainable parameters\n",
    "    static : pytree\n",
    "        Static (non-trainable) components\n",
    "    \"\"\"\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    class ResistanceModel(eqx.Module):\n",
    "        layers: list\n",
    "        num_classes: int\n",
    "        \n",
    "        def __init__(self, num_classes: int, key):\n",
    "            self.num_classes = num_classes\n",
    "            k1, k2, k3 = jax.random.split(key, 3)\n",
    "            hidden_dim = 16  # Small network to prevent overfitting\n",
    "            \n",
    "            self.layers = [\n",
    "                nn.Linear(num_classes, hidden_dim, key=k1),\n",
    "                jax.nn.relu,\n",
    "                nn.Linear(hidden_dim, hidden_dim, key=k2),\n",
    "                jax.nn.relu,\n",
    "                nn.Linear(hidden_dim, 1, key=k3),\n",
    "            ]\n",
    "        \n",
    "        def __call__(self, x):\n",
    "            \"\"\"Map one-hot feature to positive resistance.\"\"\"\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "            return jnp.exp(x) + 1e-3  # Ensure positive resistance\n",
    "    \n",
    "    model = ResistanceModel(num_classes, key)\n",
    "    params, static = eqx.partition(model, eqx.is_inexact_array)\n",
    "    return model, params, static\n",
    "\n",
    "# Initialize model\n",
    "model, params, static = build_model(len(unique_classes))\n",
    "print(f\"Model initialized with {len(unique_classes)} land-cover classes\")\n",
    "print(f\"Trainable parameters: {sum(p.size for p in jax.tree_util.tree_leaves(params))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b4ca0",
   "metadata": {},
   "source": [
    "### Visualize Initial Resistance Prediction\n",
    "\n",
    "Before training, the model produces random resistance values based on the initialization. This serves as a baseline for comparison after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1512f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model to all pixels via vmap\n",
    "model_vmapped = jax.vmap(jax.vmap(model, in_axes=0), in_axes=0)\n",
    "initial_resistance = model_vmapped(features_onehot).squeeze()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "im = ax.imshow(initial_resistance, cmap=\"RdYlGn_r\")\n",
    "ax.set_title(\"Initial Resistance Prediction (Random)\", fontsize=14, pad=20)\n",
    "ax.axis(\"off\")\n",
    "plt.colorbar(im, ax=ax, label=\"Resistance\", shrink=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Resistance range: [{initial_resistance.min():.3f}, {initial_resistance.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b61c48",
   "metadata": {},
   "source": [
    "## Define Loss Function and Training Setup\n",
    "\n",
    "The **loss function** measures the discrepancy between predicted resistance distances and observed genetic distances:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i,j} (d_{\\text{genetic}}^{ij} - d_{\\text{resistance}}^{ij})^2$$\n",
    "\n",
    "**Training procedure:**\n",
    "1. Model predicts resistance for each pixel\n",
    "2. Build GridGraph with predicted resistance\n",
    "3. Compute pairwise resistance distances between sites\n",
    "4. Compare to genetic distances via MSE\n",
    "5. Backpropagate gradients and update model parameters\n",
    "\n",
    "We use **train/test split** on pairwise distances to validate generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def loss_fn(params, args):\n",
    "    \"\"\"Compute squared error between predicted and target distances.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : pytree\n",
    "        Trainable model parameters\n",
    "    args : tuple\n",
    "        (static, features, target_flat_train, tri_i_train, tri_j_train)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        Mean squared error on training pairs\n",
    "    \"\"\"\n",
    "    static, features, target_flat_train, tri_i_train, tri_j_train = args\n",
    "    \n",
    "    # Reconstruct model and predict resistance surface\n",
    "    model = eqx.combine(params, static)\n",
    "    model_vmapped = jax.vmap(jax.vmap(model, in_axes=0), in_axes=0)\n",
    "    resistance = model_vmapped(features).squeeze()\n",
    "    \n",
    "    # Build graph and compute resistance distances\n",
    "    grid = GridGraph(resistance, fun=lambda x, y: (x + y) / 2)\n",
    "    predicted_distances = DISTANCE_FUN(grid, nodes=target_nodes)\n",
    "    \n",
    "    # Extract training pairs and compute loss\n",
    "    pred_flat_train = predicted_distances[tri_i_train, tri_j_train]\n",
    "    return ((target_flat_train - pred_flat_train) ** 2).mean()\n",
    "\n",
    "print(\"Loss function defined and JIT-compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fca247a",
   "metadata": {},
   "source": [
    "### Prepare Training and Test Sets\n",
    "\n",
    "We split pairwise distances (upper triangle of distance matrix) into 80% training and 20% test sets. This allows us to evaluate whether the model learns generalizable resistance patterns rather than overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de211b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract upper triangle indices (all unique pairs)\n",
    "n_sites = genetic_distances.shape[0]\n",
    "tri_i_all, tri_j_all = np.triu_indices(n_sites, k=1)\n",
    "target_flat_all = np.asarray(genetic_distances)[tri_i_all, tri_j_all]\n",
    "\n",
    "print(f\"Total pairwise distances: {len(target_flat_all)}\")\n",
    "\n",
    "# Train/test split (80/20)\n",
    "(\n",
    "    target_flat_train,\n",
    "    target_flat_test,\n",
    "    tri_i_train,\n",
    "    tri_i_test,\n",
    "    tri_j_train,\n",
    "    tri_j_test,\n",
    ") = train_test_split(\n",
    "    target_flat_all,\n",
    "    tri_i_all,\n",
    "    tri_j_all,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Convert to JAX arrays for training\n",
    "tri_i_train = jnp.array(tri_i_train)\n",
    "tri_j_train = jnp.array(tri_j_train)\n",
    "tri_i_test = np.array(tri_i_test)\n",
    "tri_j_test = np.array(tri_j_test)\n",
    "\n",
    "print(f\"Training pairs: {len(target_flat_train)}\")\n",
    "print(f\"Test pairs: {len(target_flat_test)}\")\n",
    "\n",
    "# Sanity check: compute initial loss\n",
    "initial_loss = loss_fn(\n",
    "    params, \n",
    "    (static, features_onehot, target_flat_train, tri_i_train, tri_j_train)\n",
    ")\n",
    "print(f\"\\nInitial training loss: {initial_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153fc00a",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "We use **L-BFGS** optimization (limited-memory Broyden-Fletcher-Goldfarb-Shanno), a quasi-Newton method that's particularly efficient for small-to-medium parameter spaces. L-BFGS uses gradient information to approximate the Hessian matrix and identify optimal parameter updates without storing the full Hessian. This approach offers several advantages: it achieves faster convergence than stochastic gradient descent for smooth objectives, provides a memory-efficient approximation of second-order information, and is well-suited to the differentiable landscape models we're working with. The optimization typically takes several minutes depending on graph size and the number of iterations required for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db11870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure L-BFGS optimizer\n",
    "solver = optx.LBFGS(\n",
    "    rtol=1e-5,  # Relative tolerance for convergence\n",
    "    atol=1e-5,  # Absolute tolerance\n",
    "    verbose=frozenset({\"loss\"})  # Print loss during optimization\n",
    ")\n",
    "\n",
    "print(\"Starting optimization...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_train_time = time.time()\n",
    "\n",
    "# Run optimization\n",
    "opt_solution = optx.minimise(\n",
    "    loss_fn,\n",
    "    solver,\n",
    "    params,\n",
    "    args=(static, features_onehot, target_flat_train, tri_i_train, tri_j_train),\n",
    "    max_steps=MAX_STEPS,\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_train_time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n✓ Training completed in {training_time:.2f} seconds\")\n",
    "print(f\"Final loss: {opt_solution.value:.6f}\")\n",
    "print(f\"\\nOptimization statistics:\")\n",
    "print(opt_solution.stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0ed2a",
   "metadata": {},
   "source": [
    "### Visualize Fitted Resistance Surface\n",
    "\n",
    "After training, we can visualize the learned resistance patterns across the landscape. High-resistance areas (shown in red) represent barriers that impede gene flow, while low-resistance areas (shown in green) facilitate connectivity between populations. When interpreting these patterns, compare the resistance surface to the original land-cover map to understand which specific habitat types facilitate or impede movement. The learned resistance values should reflect ecological realism—for example, showing roads or urban areas as barriers—and the spatial patterns should align with what's known about the species' ecology and dispersal behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd42b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fitted model to landscape\n",
    "fitted_model = eqx.combine(opt_solution.value, static)\n",
    "fitted_vmapped = jax.vmap(jax.vmap(fitted_model, in_axes=0), in_axes=0)\n",
    "fitted_resistance = fitted_vmapped(features_onehot).squeeze()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Initial (random) resistance\n",
    "im1 = ax1.imshow(initial_resistance, cmap=\"RdYlGn_r\")\n",
    "ax1.set_title(\"Initial Resistance (Random)\", fontsize=13, pad=15)\n",
    "ax1.axis(\"off\")\n",
    "plt.colorbar(im1, ax=ax1, shrink=0.6, label=\"Resistance\")\n",
    "\n",
    "# Fitted resistance\n",
    "im2 = ax2.imshow(fitted_resistance, cmap=\"RdYlGn_r\")\n",
    "ax2.set_title(\"Fitted Resistance (Trained)\", fontsize=13, pad=15)\n",
    "ax2.axis(\"off\")\n",
    "plt.colorbar(im2, ax=ax2, shrink=0.6, label=\"Resistance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Fitted resistance range: [{fitted_resistance.min():.3f}, {fitted_resistance.max():.3f}]\")\n",
    "print(f\"Mean resistance: {fitted_resistance.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c4dec",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "\n",
    "We assess model quality using two complementary metrics. **R² (coefficient of determination)** measures the proportion of variance in genetic distances explained by the model, with 1.0 indicating a perfect fit. **RMSE (root mean squared error)** quantifies the average prediction error in the original measurement units, providing an interpretable scale of accuracy. By reporting these metrics separately for training and test sets, we can detect overfitting—where the model memorizes training data rather than learning generalizable patterns. Good generalization is indicated by similar performance on both sets, suggesting the learned resistance patterns capture true landscape-genetic relationships rather than noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted distances using fitted resistance\n",
    "pred_grid = GridGraph(fitted_resistance, fun=lambda x, y: (x + y) / 2)\n",
    "pred_distances = DISTANCE_FUN(pred_grid, nodes=target_nodes)\n",
    "\n",
    "genetic_np = np.asarray(genetic_distances)\n",
    "pred_np = np.asarray(pred_distances)\n",
    "\n",
    "# Extract predictions for train and test pairs\n",
    "train_pred = pred_np[tri_i_train, tri_j_train]\n",
    "test_pred = pred_np[tri_i_test, tri_j_test]\n",
    "train_target = target_flat_train\n",
    "test_target = target_flat_test\n",
    "\n",
    "# Compute metrics\n",
    "r2_train = r2_score(train_target, train_pred)\n",
    "r2_test = r2_score(test_target, test_pred)\n",
    "rmse_train = np.sqrt(mean_squared_error(train_target, train_pred))\n",
    "rmse_test = np.sqrt(mean_squared_error(test_target, test_pred))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Metric':<15} {'Training':<15} {'Test':<15}\")\n",
    "print(\"-\"*45)\n",
    "print(f\"{'R²':<15} {r2_train:>14.3f} {r2_test:>14.3f}\")\n",
    "print(f\"{'RMSE':<15} {rmse_train:>14.4f} {rmse_test:>14.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for overfitting\n",
    "if r2_train - r2_test > 0.2:\n",
    "    print(\"\\n⚠️  Warning: Possible overfitting detected (train R² >> test R²)\")\n",
    "elif r2_test >= 0.5:\n",
    "    print(\"\\n✓ Good generalization: Model explains genetic patterns well\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Moderate fit: Consider model refinement or more data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b666a29e",
   "metadata": {},
   "source": [
    "### Predicted vs. Observed: Scatterplot\n",
    "\n",
    "The scatterplot visualizes how well predicted resistance distances match observed genetic distances:\n",
    "\n",
    "- **Points near diagonal**: Good predictions\n",
    "- **Training vs. test**: Different colors show generalization\n",
    "- **Systematic deviations**: May indicate model misspecification\n",
    "\n",
    "An ideal model would have all points falling on the 1:1 line with similar scatter for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 8))\n",
    "\n",
    "# Plot training and test predictions\n",
    "ax.scatter(\n",
    "    train_pred, train_target, \n",
    "    s=60, alpha=0.6, \n",
    "    edgecolor=\"none\", \n",
    "    label=\"Training\",\n",
    "    c='#2E86AB'\n",
    ")\n",
    "ax.scatter(\n",
    "    test_pred, test_target, \n",
    "    s=80, alpha=0.8, \n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1,\n",
    "    label=\"Test\",\n",
    "    c='#A23B72'\n",
    ")\n",
    "\n",
    "# 1:1 reference line\n",
    "min_val = min(pred_np.min(), genetic_np.min())\n",
    "max_val = max(pred_np.max(), genetic_np.max())\n",
    "ax.plot(\n",
    "    [min_val, max_val], [min_val, max_val], \n",
    "    \"k--\", linewidth=2, \n",
    "    alpha=0.5,\n",
    "    label=\"Perfect fit (1:1)\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Predicted Resistance Distance\", fontsize=12)\n",
    "ax.set_ylabel(\"Observed Genetic Distance (Fst)\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Inverse Landscape Model: Predicted vs. Observed\", \n",
    "    fontsize=14, \n",
    "    pad=20\n",
    ")\n",
    "\n",
    "# Add metrics box\n",
    "textstr = (\n",
    "    f\"Training\\n\"\n",
    "    f\"  R² = {r2_train:.3f}\\n\"\n",
    "    f\"  RMSE = {rmse_train:.4f}\\n\\n\"\n",
    "    f\"Test\\n\"\n",
    "    f\"  R² = {r2_test:.3f}\\n\"\n",
    "    f\"  RMSE = {rmse_test:.4f}\"\n",
    ")\n",
    "ax.text(\n",
    "    0.05, 0.95,\n",
    "    textstr,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=dict(\n",
    "        boxstyle=\"round\", \n",
    "        facecolor=\"white\", \n",
    "        alpha=0.9, \n",
    "        edgecolor=\"gray\",\n",
    "        linewidth=1.5\n",
    "    ),\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"lower right\", fontsize=11, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3, linestyle=':')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0841757",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Methodological Insights\n",
    "\n",
    "**Inverse modeling** enables data-driven inference of landscape resistance patterns directly from genetic data, reversing the traditional approach of assuming resistance values *a priori*. The power of **automatic differentiation** through JAX makes gradient-based optimization tractable even for complex spatial models with thousands of parameters. **Neural networks** provide flexible parameterization that can capture nonlinear relationships between land-cover and resistance while maintaining interpretability through the one-hot encoding of discrete habitat classes. Finally, **train/test splits** are essential for validating that learned resistance patterns represent generalizable landscape-genetic relationships rather than memorized training data.\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "This approach has direct utility for **conservation planning**, where identifying which landscape features most facilitate or impede gene flow guides restoration priorities. The fitted resistance surfaces support **corridor design** by revealing optimal pathways for habitat connectivity interventions. Repeating the analysis for different taxa yields **species-specific models** that quantify how different organisms respond to the same landscape, informing multi-species conservation strategies. The trained models also enable **scenario evaluation**, predicting how proposed land-use changes would affect genetic connectivity before interventions are implemented.\n",
    "\n",
    "### Extensions\n",
    "\n",
    "Several extensions could enhance the approach. Comparing **multiple distance metrics** (LCP, Resistance, RSP) would reveal whether genetic patterns better match single-path or multi-path connectivity models. Incorporating additional **environmental covariates** such as climate variables and topography beyond land-cover could improve predictions in heterogeneous landscapes. **Hierarchical models** that explicitly account for population structure and demographic history would separate landscape effects from other evolutionary processes. Finally, **uncertainty quantification** through bootstrap resampling or Bayesian approaches would provide confidence intervals on learned resistance values.\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "Several practical considerations affect model performance. Loss values exceeding 0.002 often indicate poor convergence and may require adjusting learning rates or network architecture. The deliberately small hidden layer (16 units) helps prevent overfitting when training on limited genetic data typical of empirical studies. The coarsening factor represents a trade-off between spatial resolution and computational efficiency that should be tuned based on species dispersal scales. Most importantly, results should always be validated against independent ecological knowledge about species behavior and habitat use to ensure biological realism."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
